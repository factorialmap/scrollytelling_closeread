---
title: "Análise Fatorial Exploratória (EFA)"
format:
  closeread-html:
    css: new.css
    code-tools: false
    fig-format: svg
    toc: false
    linkcolor: "#1abc9c"
---

### [**O que é a EFA**]{style="color: #5AC8BE ;"}

A Análise Fatorial Exploratória (EFA) é uma técnica estatística multivariada usada para identificar a estrutura subjacente (fatores latentes) em um conjunto de variáveis observadas. Ela busca agrupar variáveis correlacionadas entre si em fatores comuns, revelando padrões ocultos nos dados.

É chamada de "exploratória" porque não parte de uma hipótese prévia sobre o número ou a natureza dos fatores, ao contrário da Análise Fatorial Confirmatória (CFA), que testa modelos específicos.

![Exemplode Análise fatorial exploratória](assets/images/efa_figure.png)

### [**Qual o objetivo?**]{style="color: #5AC8BE ;"}

O principal objetivo da EFA é reduzir a dimensionalidade dos dados e descobrir estruturas latentes que explicam as correlações entre variáveis. Isso permite:

-   Identificar agrupamentos naturais de variáveis.
-   Compreender melhor os construtos teóricos por trás dos dados.
-   Preparar modelos mais simples e interpretáveis.
-   Apoiar o desenvolvimento de escalas e instrumentos de medida (como questionários).

### [**De onde vem?**]{style="color: #5AC8BE ;"}

Ela surgiu no campo da psicometria e da psicologia, especialmente para entender traços de personalidade e habilidades cognitivas. Foi desenvolvida como uma resposta à necessidade de compreender fenômenos complexos que não podiam ser observados diretamente, mas inferidos a partir de múltiplas variáveis.

Ela se baseia em conceitos de correlação, covariância e álgebra linear, especialmente decomposição de matrizes.

### [**Como fazer?**]{style="color: #5AC8BE ;"}

Aqui está um passo a passo prático para realizar uma EFA em R:

1.  Preparar os dados verificando se há dados faltantes, certifique-se de que as variáveis são numéricas e correlacionáveis. Se forem dicotômicas será necessário utilizr abordagem específica que será explicada nos exemplos mais abaixo.

2.  Verificar a adequação dos dados usando teste de Kaiser-Meyer-Olkin (KMO) e ou teste de esfericidade de Bartlett.

3.  Escolher o número de fatores usando scree plot (gráfico de sedimentação), critério de Kaiser (autovalores \> 1), e análise paralela.

4.  Executar a EFA usando a função `factanal()` ou pacotes como `psych` (`fa()`), escolher o método de extração (ex: máxima verossimilhança, componentes principais) e escolher o tipo de rotação (ex: varimax, oblimin).

5.  Interpretar os resultados analisando as cargas fatoriais, verificando comunalidades e variância explicada e validar a consistência dos fatores.

6.  Refinar o modelo eliminando variáveis com baixa carga ou comunalidade e reexecutando a análise se necessário.

Para praticar, iremos usar três exemplos sendo o primeiro com dados contínuos o segundo com com dados categóricos dicotômicos, e terceiro com dados mistos(categóricos dicotômicos e ordinais e numéricos).


:::: {.cr-section layout="sidebar-left"}

::: {focus-on = "cr-case1_consumidor"}
## Caso1: Comportamento de consumo com variáveis contínuas

Imagine que estou conduzindo uma análise fatorial em um questionário com 12 variáveis que representam respostas sobre comportamento de consumo.

Eu aplico um questionário com 12 questões e a variância dessas 12 variáveis podem ser amplamente explicada por um número menor de clusters subjacentes chamados de **fatores**, ou seja, cada fator, consiste em muitas variáveis.

:::

Vamos fazer a simulação no R, e pra isso, precisaremos carregar os pacotes necessários. Caso os pacotes não estejam instalados, para instalar é simples basta usar a função `install.packages("nome_do_pacote_aqui")`.  @cr-case1_packages
  
Vamos criar um conjunto de dados com 12 variáveis que representam respostas a um questionário sobre comportamento de consumo, agrupadas em 3 fatores latentes. [@cr-case1_data]{scale-by="100%,100%" scale-by="1"} 

A função `set.seed` nos ajuda a manter os dados randomicos fixos, permitindo que o experimento possa ser reproduzido em outra maquina, mas gerando os mesmos resultados. [@cr-case1_data]{highlight='1-2'}

Na sequencia geramos 300 observações de dados e depois três fatores com 300 observações cada. [@cr-case1_data]{highlight='4-8'}

Quando simulamos variáveis contínuas para representar respostas de um questionário, estamos aproximando o comportamento de escalas de Likert que foram normalizadas ou tratadas como contínuas. As escalas de Likert são ordinais, geralmente com 5 ou 7 pontos, como: `1 = Discordo totalmente`, `2 = Discordo`, `3 = Neutro`, `4 = Concordo`, `5 = Concordo totalmente`. [@cr-case1_data]{highlight='10-22}
  
Apesar de serem categorias ordenadas, na prática estatística (especialmente em psicometria e ciências sociais), elas são frequentemente tratadas como variáveis contínuas, principalmente quando há muitos itens (variáveis), a escala tem mais de 4 pontos e os dados são usados em modelos paramétricos como EFA, regressão, etc.
  
Na simulação, as variáveis foram geradas como contínuas com ruído, o que simula o comportamento de respostas em Likert. Isso equivale a dizer que os dados passaram por uma transformação ou suavização como por exemplo padronização (z-score), conversão para escala contínua (ex: média de itens), tratamento como intervalares para fins de modelagem.

::: {#cr-case1_consumidor}
![Consumidor](assets/images/consumidor.png)
:::

::: {#cr-case1_packages}
```{r case1_packages, warning=FALSE, message=FALSE, echo=TRUE}

# Carregue os pacotes ou instale-os no caso de ainda não ter.
library(tidyverse)
library(psych)
library(GPArotation)
library(gt)
library(parameters)
```
:::

::: {#cr-case1_data}
```{r case1_data, warning=FALSE, message=FALSE, echo=TRUE}

# este comando permite replicarmos os dados randomicos
set.seed(123)

# Simulando 300 pontos de dados com 3 fatores latentes
n <- 300
fator1 <- rnorm(n)
fator2 <- rnorm(n)
fator3 <- rnorm(n)

data_consumo <- tibble(
  consumo_online1 = fator1 + rnorm(n, 0, 0.5),
  consumo_online2 = fator1 + rnorm(n, 0, 0.5),
  consumo_online3 = fator1 + rnorm(n, 0, 0.5),
  fidelidade_marca1 = fator2 + rnorm(n, 0, 0.5),
  fidelidade_marca2 = fator2 + rnorm(n, 0, 0.5),
  fidelidade_marca3 = fator2 + rnorm(n, 0, 0.5),
  preocupação_preço1 = fator3 + rnorm(n, 0, 0.5),
  preocupação_preço2 = fator3 + rnorm(n, 0, 0.5),
  preocupação_preço3 = fator3 + rnorm(n, 0, 0.5),
  misc1 = rnorm(n),
  misc2 = rnorm(n),
  misc3 = rnorm(n))
```

:::

::::

:::: {.cr-section layout="sidebar-left"}

@cr-case1_kmo Neste passo, faremos a verificação e adequação dos dados usando funções como KMO e Teste de esfericidade de Bartlett. No código ao lado entenda o simbolo `|>` como `e então`. Você verá este simbolo frequentemente no R. Usamos a função `KMO` **e então**, transformamos os dados em um data frame usando a função `enframe` **e então**, transformamos novamente o dataframe em uma tabela usando a função `gt::gt()`.


O teste **KMO (Kaiser-Meyer-Olkin)** é uma métrica avalia a adequação dos nossos dados para a Análise Fatorial. Ele mede a proporção da variância entre as variáveis que pode ser considerada comum. Valores mais próximos de 1 indicam que as correlações entre os pares de variáveis são compactas e que a análise fatorial produzirá fatores distintos e confiáveis. Essencialmente, ele responde se suas variáveis formam um conjunto coeso. [@cr-case1_kmo]{highlight='1-2}

Já o **Teste de esfericidade de Bartlett** verifica a hipótese de que a sua matriz de correlação é uma matriz identidade, o que significaria que as variáveis não são correlacionadas. Um resultado estatisticamente significativo (geralmente p < 0.05) é desejado, pois rejeita essa hipótese e confirma que existe correlação suficiente entre as variáveis para justificar a aplicação da análise fatorial. [@cr-case1_bartlett]

@cr-case1_plot Aqui, faremos a escolha do número de fatores. O gráfico `Scree Plot` nos ajudará a visualizar o número ideal de fatores com base nos autovalores. 

O gráfico gerado pela função `fa.parallel()` é um Scree Plot com Análise Paralela. Ele ajuda a separar os fatores que são realmente significativos do "ruído" aleatório nos dados.

A linha azul (com "triangulo") representa os autovalores (**a importância**) de cada fator extraído dos dados reais.
A linha vermelha (tracejada) representa a média dos autovalores que seriam esperados em um conjunto de dados aleatórios, sem estrutura real, mas com o mesmo número de variáveis e observações que o nosso conjunto de dados.

**A regra é simples**: você deve reter o número de fatores cujos autovalores (pontos na linha azul) estão acima dos autovalores correspondentes dos dados aleatórios (a linha vermelha). Portanto, o fato de você ter 3 pontos da linha azul acima da linha vermelha é uma forte indicação de que existem 3 fatores latentes significativos em nossos dados. Os fatores seguintes estão abaixo da linha vermelha, o que sugere que eles provavelmente representam apenas variação aleatória (ruído) e não devem ser incluídos no nosso modelo.

::: {#cr-case1_kmo}
```{r case1_kmo, warning=FALSE, message=FALSE, echo=TRUE}

# Teste KMO
KMO(cor(data_consumo))$MSAi |>
  enframe(name = "variable", value = "value") |> 
  gt::gt()

```

:::

::: {#cr-case1_bartlett}

```{r case1_bartlett, warning=FALSE, message=FALSE, echo=TRUE}
# Teste de esfericidade de Bartlett
cortest.bartlett(cor(data_consumo), n = n) |> as_tibble() |> gt::gt()

```

:::


::: {#cr-case1_plot}
```{r case1_plot, warning=FALSE, message=FALSE, echo=TRUE}

# Scree plot
fa.parallel(data_consumo, fa = "fa", n.obs = n)

```

:::

::::

:::: {.cr-section layout="sidebar-left"}

@cr-case1_fa Agora, vamos executar da analise fatorial exploratória.

::: {#cr-case1_fa}
```{r case1_fa, warning=FALSE, message=FALSE, echo=TRUE}

# Análise fatorial com 3 fatores e rotação varimax
efa_result <- 
  fa(data_consumo, 
    nfactors = 3,
    rotate = "varimax", 
    fm = "ml")

```

:::

::::


:::: {.cr-section layout="sidebar-left"}

 @cr-case1_fa_res Vamos analisar os resultados apresentados na análise fatorial exploratória. Usamos a função `model_parameters` que irá extrair os parâmetros do modelo (**factor loadings ou cargas fatoriais**), que são os coeficiêntes de correlação entre as variáveis originais(e.g. consumo online) e os fatores (e.g. ML1, ML2, ML3). Note na tabela que elas possuem um intervalo de -1 a 1. (Se aparecer valores como este -1.71e-03 por exemplo, não se preocupe, são notações científicas que representam valores muito baixos).

Podemos observar na tabela que as variáveis `consumo_online1`,`consumo_online2` e `consumo_online3` tem cargas fatoriais altas no fator **ML3** (0.91,0.87,0,87). Isso sugere que o **ML3** representa o construto `Consumo Online`.

Quando analisamos as variáveis `fidelidade_marca1`, `fidelidade_marca2` e `fidelidade_marca3` podemos notar que há uma carga grande no fator **ML2**. Neste caso, este fator representa o constructo `Fidelidade com a Marca`.

Ao observarmos as variáveis `preocupação_preço1`, `preocupação_preço2` e `preocupação_preço3` possuem cargas altas no fator **ML1**. Este fator representa o constructo `Preocupação com o Preço`.

Por fim, as variáveis `misc` tem cargas baixas em todos os fatores, indicado que não se encaixam bem em nenhum desses constructos e poderiam ser removidas da análise.

A coluna **Uniqueness** ou unicidade é a proporção da variância de uma variável que não é explicada pelos fatores.

Um valor de unicidade baixo (ex: para consumo_online1 é 0.16) significa que os fatores explicam uma grande parte da variância daquela variável (1 - 0.16 = 84%). Isso é bom pois significa que a variável está bem representada pela solução fatorial. 

Já Um valor de unicidade alto (ex: para misc1 é 0.99) significa que os fatores explicam muito pouco de sua variância (1 - 0.99 = 1%). Isso sugere que a variável não pertence ao grupo das outras e pode medir algo completamente diferente.

O resumo na parte inferior informa o quanto da variância total de todas as variáveis originais é capturado pela sua solução fatorial. "The 3 latent factors... accounted for 59.86% of the total variance..." (Os 3 fatores latentes... explicaram 59.86% da variância total...). Isso significa que nosso modelo de três fatores simplifica com sucesso as 12 variáveis originais, ainda explicando quase 60% de suas informações combinadas. Este é um resultado razoavelmente bom. 

As porcentagens individuais (ML2 = 20.13%, ML1 = 20.10%, ML3 = 19.62%) mostram quanta variância cada fator explica. Neste caso, os fatores têm importância aproximadamente igual.

::: {#cr-case1_fa_res}
```{r cr-case1_fa_res, warning=FALSE, message=FALSE, echo=TRUE}

# mostrando os resultados da analise fatorial exploratória - loadings
parameters::model_parameters(efa_result)

```

:::

::::

:::: {.cr-section layout="sidebar-left"}


@cr-case1_results Outra forma de extrair os loadings do modelo.

::: {#cr-case1_results}
```{r case1_results_loadings, warning=FALSE, message=FALSE, echo=TRUE}

# Cargas fatoriais
efa_result$loadings |>
   enframe(name = "variable", value = "value")

```

:::

@cr-case1_results_communality Caso precise de outros resumos estatísticos como **communality** usamos aqui a função **enframe** para retornar os dados em formato de tabela.  A comunalidade é o oposto da Unicidade ou seja, (1 - Unicidade) e representa a proporção da variância que é explicada pelos fatores.



::: {#cr-case1_results_communality}
```{r case1_results_communality, warning=FALSE, message=FALSE, echo=TRUE}
# Comunalidades
efa_result$communality |> 
  enframe(name = "variable", value = "value")

```

:::

@cr-case1_results_explained Assim como no resumo da tabela que vimos anteriormente, também é possível listar os valores da variância explicada. 

O **SS loadings** ou soma dos quadrados das cargas, é o autovalor (eigenvalue) de cada fator. Ele representa a "importância" ou a quantidade total de variância que cada fator consegue explicar a partir de todas as variáveis. Um valor maior significa que o fator é mais relevante.

O **Proportion Var** ou proporção da variância, é a porcentagem da variância total que cada fator explica individualmente. Por exemplo, um valor de 0.20 significa que aquele fator sozinho explica 20% da variância de todos os nossos dados.

A **Cumulative Var** ou variância cumulativa, é a soma acumulada da Proportion Var. Mostra a porcentagem total de variância explicada pelo conjunto de fatores até aquele ponto. O último valor desta linha indica o poder explicativo total do seu modelo (ex: 0.59 significa que todos os fatores juntos explicam 59% da variância total).

A **Proportion Explained** ou proporção explicada, mostra a porcentagem da variância explicada que cada fator representa. A métrica foca apenas na parte que o modelo conseguiu capturar (e.g. se nosso modelo explica 60% da variância total, e o proportion explained de um fator é 0.34, isso significa que este fator é responsável por 34% desses 60% explicados).

O **Cumulative Proportion** ou proporção cumulativa, é a soma acumulada da proportion explained. O valor final será sempre 1 (ou 100%), pois representa a totalidade da variância que foi explicada pelo seu conjunto de fatores(e.g. se os dois primeiros fatores têm uma cumulative proportion de 0.67, isso significa que juntos eles respondem por 67% de toda a variância que o seu modelo conseguiu explicar).


::: {#cr-case1_results_explained}
```{r case1_results_explained, warning=FALSE, message=FALSE, echo=TRUE}
# Variância explicada
efa_result$Vaccounted |> 
  enframe(name = "variable", value = "value") |> 
  gt::gt()

```

:::

::::

:::: {.cr-section layout="sidebar-left"}

::: {focus-on = "cr-case2_sintomas"}
## Caso2 - Sintomas de saúde com variáveis dicotômicas

Imagine que estou conduzindo uma análise fatorial para investigar uma possível estrutura latente para 10 sintomas definidos apenas por variáveis dicotômicas (0 = ausente/não, 1 = presente/sim). Preciso realizar uma análise fatorial exploratória apenas com variáveis categóricas para isso preciso saber qual a melhor abordagem e matriz de correlação utilizar.
:::

::: {#cr-case2_sintomas}
![Sintomas de saúde](assets/images/sintomas.png)
:::

Quando lidamos com variáveis dicotômicas (0 = ausente, 1 = presente) em uma análise fatorial exploratória, precisamos adaptar a abordagem tradicional, pois a matriz de correlação padrão (Pearson) não é apropriada para variáveis categóricas.

Para casos como este, a correlação tetra-córica é mais adequada. Ela é ideal para variáveis dicotômicas que representam uma versão categorizada de uma variável contínua subjacente, pois estima a correlação entre essas variáveis como se fossem contínuas e normalmente distribuídas.

@cr-case2_packages Vamos carregar os pacotes necessários para análise fatorial exploratória.


@cr-case2_data Vamos gerar os dados para a simulação.

::: {#cr-case2_packages}
```{r case_2_packages, warning=FALSE, message=FALSE, echo=TRUE}

# carregar os pacotes necessários
library(tidyverse)  # manipulação de dados
library(psych) # Análise fatorial exploratória
library(polycor) # Correlação tetra-córica

```

:::

::: {#cr-case2_data}
```{r case2_data, warning=FALSE, message=FALSE, echo=TRUE}

# este comando permite replicarmos os dados randomicos
set.seed(123)

# Simulando 300 pontos de dados com 3 fatores latentes
n <- 300

# Gerar os dados dicotomicos para a simulação
data_sintomas <- tibble(
  febre = rbinom(n, 1, 0.6),
  tosse = rbinom(n, 1, 0.5),
  fadiga = rbinom(n, 1, 0.4),
  dor_cabeca = rbinom(n, 1, 0.5),
  falta_ar = rbinom(n, 1, 0.3),
  dor_muscular = rbinom(n, 1, 0.4),
  perda_olfato = rbinom(n, 1, 0.2),
  perda_paladar = rbinom(n, 1, 0.2),
  congestao = rbinom(n, 1, 0.3),
  nauseas = rbinom(n, 1, 0.25))
```

:::

::::

:::: {.cr-section layout="sidebar-left"}

@cr-case2_tetra Vamos gerar a matriz de correlação.

@cr-case2_plot Vamos plotar a matriz de correlação.


::: {#cr-case2_tetra}
```{r case_2_tetra, warning=FALSE, message=FALSE, echo=TRUE}

# Matriz de correlação tetra-córica
matriz_tetra <- tetrachoric(data_sintomas)$rho

```

:::

::: {#cr-case2_plot}
```{r case_2_plot, warning=FALSE, message=FALSE, echo=TRUE}

fa.parallel(matriz_tetra, n.obs = n, fa = "fa")

```

:::

::::

:::: {.cr-section layout="sidebar-left"}

@cr-case2_fa Vamos executar da analise fatorial exploratória.


::: {#cr-case2_fa}
```{r case_2_fa, warning=FALSE, message=FALSE, echo=TRUE}

efa_sintomas <- 
  fa(matriz_tetra, 
    nfactors = 2,
    rotate = "varimax", 
    fm = "ml")

```

:::

::: {#cr-case2_fa_results}
```{r case2_fa_results, warning=FALSE, message=FALSE, echo=TRUE}

# explorar os resultados da analise fatorial exploratória
efa_sintomas$loadings |> 
  enframe(name = "variable", value = "value") |>
  gt::gt()

```

:::

::::

:::: {.cr-section layout="sidebar-left"}

@cr-case2_loading Vamos fazer a interpretação dos resultados

::: {#cr-case2_loading}
```{r case2_loading, warning=FALSE, message=FALSE, echo=TRUE}

# Cargas fatoriais
efa_sintomas$loadings |> 
  enframe(name = "variable", value = "value") |>
  gt::gt()

```

:::

@cr-case2_results_explained Nesta etapa fazemos a análise de variância explicada.

::: {#cr-case2_results_explained}

```{r case2_results_explained, warning=FALSE, message=FALSE, echo=TRUE}
# Variância explicada
efa_sintomas$Vaccounted |> 
  enframe(name = "variable", value = "value") |>
  gt::gt()

```

:::

::::

------------------------------------------------------------------------

:::: {.cr-section layout="sidebar-left"}

::: {focus-on = "cr-case3_data"}
## Caso3 - Sintomas de saúde com variáveis mistas

Quando temos variáveis mistas em um questionário (algumas dicotômicas e outras em escala de Likert), a escolha da abordagem para a análise fatorial exploratória precisa considerar a natureza dos dados para garantir resultados válidos.

A correlação policórica/tetra-córica pode ser mais adequada para estes casos sendo que:

-   As variáveis dicotômicas (0/1) devem ser tratadas com correlação tetra-córica.
-   As variáveis ordinais (Likert) devem ser tratadas com correlação policórica.
-   Tratar todas como contínuas (usando correlação de Pearson) pode distorcer os resultados, especialmente se houver muitas variáveis dicotômicas ou escalas curtas (ex: Likert de 3 ou 5 pontos).
:::

O pacote polycor oferece a função hetcor() que calcula uma matriz de correlação heterogênea, combinando `pearson` para variáveis contínuas, `policórica` para variáveis ordinais, `tetra-córica` para variáveis dicotômicas. [@cr-case3_data]{highlight='1'}

Vamos gerar os dados para a simulação. [@cr-case3_data]{scale-by="100%,100%" scale-by="1"}

Vamos executar a analise [@cr-case3_code]{scale-by="100%,100%" scale-by="1"}


::: {#cr-case3_data}
```{r case3_data, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}

library(polycor)

# Suponha que você tenha um data frame com variáveis mistas
# Algumas dicotômicas (0/1), outras em escala de Likert (1 a 5)

# Exemplo simulado
dados_mistos <- tibble(
  febre = rbinom(300, 1, 0.6),                   # dicotômica
  tosse = rbinom(300, 1, 0.5),                   # dicotômica
  fadiga = sample(1:5, 300, replace = TRUE),     # Likert
  dor_cabeca = sample(1:5, 300, replace = TRUE), # Likert
  falta_ar = rbinom(300, 1, 0.3),                # dicotômica
  sono = sample(1:5, 300, replace = TRUE))        # Likert

# Matriz de correlação heterogênea
matriz_het <- polycor::hetcor(dados_mistos)$correlations
```

:::

::: {#cr-case3_code}
```{r case3_code, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}

library(psych)

# Escolher número de fatores (opcional)
fa.parallel(matriz_het, n.obs = 300, fa = "fa")

# Executar EFA
efa_mista <- fa(matriz_het, nfactors = 2, rotate = "varimax", fm = "ml")
print(efa_mista)

```

:::

::::


### [**Pra onde vai?**]{style="color: #5AC8BE ;"}

A análise fatorial exploratória serve como base para modelos mais avançados, como a Análise Fatorial Confirmatória (CFA) e Modelagem de Equações Estruturais (SEM) e é amplamente utilizada em diferentes áreas como:

-   **Psicologia e educação**: desenvolvimento de testes e escalas.
-   **Marketing**: segmentação de consumidores com base em comportamentos.
-   **Saúde**: identificação de padrões em sintomas ou respostas a tratamentos.
-   **Ciências sociais**: estudo de atitudes, valores e crenças.

### [**Qual resultado?**]{style="color: #5AC8BE ;"}

Após realizarmos a análise fatorial exploratória

-   Um conjunto de fatores latentes que explicam a estrutura dos dados.
-   Redução da complexidade do modelo.
-   Melhor compreensão dos constructos teóricos.
-   Um modelo mais parcimonioso e interpretável.

Esses resultados ajudam na tomada de decisão, no desenvolvimento de instrumentos de medida e na validação de teorias.